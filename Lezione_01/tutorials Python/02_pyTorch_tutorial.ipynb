{"cells":[{"cell_type":"markdown","metadata":{"id":"kKsiHOHkVLkZ"},"source":["# PyTorch Tutorial\n","(Chapter 2. Preliminaries D2L)"]},{"cell_type":"markdown","metadata":{"id":"ab98LCJxGWkG"},"source":["Index\n","\n","* [PyTorch](#PyTorch)\n","* [Tensors](#Tensors)\n","* [Operations on Tensors](#Operations-on-Tensors)\n","* [Tensors: Scenic views of storage*](#Tensors:-Scenic-views-of-storage-*)\n","* [CUDA](#CUDA)\n","\n","\n","\\* sections that can be skipped on first reading"]},{"cell_type":"markdown","metadata":{"id":"hqIfZbAVVLkd"},"source":["# PyTorch\n","\n","* Integration with the rest of the scientific libraries in Python, such as [SciPy](https://www.scipy.org), [Scikit-learn](https://scikit-learn.org), and [Pandas](https://pandas.pydata.org)\n","* Compared to NumPy arrays, PyTorch tensors perform very fast operations on graphical processing units (GPUs)\n","  * distribute operations on multiple devices or machines\n","  * keep track of the graph of computations that created them\n","  * important features when implementing a modern deep learning library."]},{"cell_type":"markdown","metadata":{"id":"oRGUvEaaGWkM"},"source":["# Tensors \n","\n","* Tensors are a specialized __data structure__ that are very similar to __arrays__ and __matrices__\n","* In __PyTorch__, we use tensors to encode the __inputs__ and __outputs__ of a model, as well as the model’s __parameters__\n","* Tensors are similar to __NumPy__ ndarrays, except that tensors can run on __GPUs__\n","* Tensors are also optimized for __automatic differentiation__\n","* The __dimensionality__ of a tensor coincides with the __number of indexes__ used to refer to scalar values within the tensor"]},{"cell_type":"markdown","metadata":{"id":"SERHbScrVLkf"},"source":["![tensors](https://raw.githubusercontent.com/giulianogrossi/imgs/main/pyTorch_tutorial_imgs/tensors.png)"]},{"cell_type":"markdown","metadata":{"id":"Dv5YFioTVLkf"},"source":["## The essence of tensors\n","\n","* Python __lists__ or __tuples__ of numbers are collections of Python objects that are __individually allocated__ in memory\n","* PyTorch __tensors__ or __NumPy arrays__ are views over (typically) contiguous __memory blocks__ containing unboxed C numeric types rather than Python objects\n","* Each element is (in general) a 32-bit (4-byte) __float__ or __int__ "]},{"cell_type":"markdown","metadata":{"id":"N_YHp6oSVLkg"},"source":["![memory](https://raw.githubusercontent.com/giulianogrossi/imgs/main/pyTorch_tutorial_imgs/memory.png)"]},{"cell_type":"markdown","metadata":{"id":"HNZm-icHVLkq"},"source":["# Tensors: Scenic views of storage \n","\n","* Values in tensors are allocated in __contiguous chunks__ of memory managed by `torch.Storage` instances\n","* A storage is a __one-dimensional array__ of numerical data: a contiguous block of memory containing numbers of a given type, i.e.\n","  * `float` (32 bits repre- senting a floating-point number)\n","  * `int64` (64 bits representing an integer)."]},{"cell_type":"markdown","metadata":{"id":"BRrhm49VVLkr"},"source":["![storage](https://raw.githubusercontent.com/giulianogrossi/imgs/main/pyTorch_tutorial_imgs/storage.png)"]},{"cell_type":"markdown","metadata":{"id":"iaUKLedjVLkr"},"source":["## Tensor metadata\n","* In order to index into a storage, tensors rely on a few pieces of information that, together with their storage, unequivocally define them:\n"," * __size__: is a tuple indicating how many elements across each dimension\n"," * __offset__: is the index in the storage corresponding to the first element in the tensor. \n"," * __stride__: is the number of elements in the storage that need to be skipped over to obtain the next element along each dimension\n"]},{"cell_type":"markdown","metadata":{"id":"yWISaD0cVLkr"},"source":["![memory](https://raw.githubusercontent.com/giulianogrossi/imgs/main/pyTorch_tutorial_imgs/stride.png)"]},{"cell_type":"markdown","metadata":{"id":"k5OOql-AGWkO"},"source":["# Initializing a Tensor\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibd4eOk-GWkN"},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","source":["1. With **$N$ sequential values** starting from 0 "],"metadata":{"id":"xuCwadCRnVwk"}},{"cell_type":"code","source":["# Tensor with sequential values \n","N = 12\n","x = torch.arange(N) #, dtype=torch.float32) #check for possible data types\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sL0Tb6pKVPHX","executionInfo":{"status":"ok","timestamp":1648803751865,"user_tz":-120,"elapsed":4,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"f9f509ec-7bea-4ccf-916e-f18e7f4a0237"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"pJfBJDrtGWkP"},"source":["2.  With **Random or Constant** values\n","    * Typically initialization are either with zeros, ones, some other constants, or numbers randomly sampled from a specific distribution. \n","        * `zeros` allows to initialize the tensors explicitly to  0 \n","        * `ones` allows to initialize the tensors explicitly to  1 \n","        * `randn` initializes the matrix randomly sampling the elements values from a standard Gaussian (normal) distribution with a mean of 0 and a standard deviation of 1\n","\n","      (``shape`` is a tuple of tensor dimensions)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QezrxKPkGWkQ"},"outputs":[],"source":["shape = (3,4)\n","zeros_tensor = torch.zeros(shape)\n","ones_tensor = torch.ones(shape)\n","rand_tensor = torch.rand(shape)\n","\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")"]},{"cell_type":"markdown","source":["3. Specifying the exact values in a **List** and then converting it TO a Tensor"],"metadata":{"id":"ox4ERUaFXfBK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8Hkh3l1GWkO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648803473729,"user_tz":-120,"elapsed":563,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"7d4ff028-269a-4d97-f514-aef6c07eda90"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2],\n","        [3, 4]])"]},"metadata":{},"execution_count":9}],"source":["#List TO tensor\n","data = [[1, 2],[3, 4]]\n","x_data = torch.tensor(data) \n","x_data"]},{"cell_type":"markdown","metadata":{"id":"NRH43kEKGWkO"},"source":["\n","4. **From NumPy** arrays TO Tensors\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoC9y7rMGWkP"},"outputs":[],"source":["# Numpy TO tensor\n","np_array = np.array(data)\n","x_np = torch.from_numpy(np_array)"]},{"cell_type":"markdown","metadata":{"id":"A9Y1kP_UGWkP"},"source":["5. From another **Tensor**\n","    * The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ty9JBAlBGWkP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648804193087,"user_tz":-120,"elapsed":384,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"9b3298ec-03b1-4152-d135-54995c04cd1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ones Tensor: \n"," tensor([[1, 1],\n","        [1, 1]]) \n","\n","Random Tensor: \n"," tensor([[0.0993, 0.3592],\n","        [0.5374, 0.9289]]) \n","\n"]}],"source":["x_ones = torch.ones_like(x_data) # retains the properties of x_data\n","print(f\"Ones Tensor: \\n {x_ones} \\n\")\n","\n","x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n","print(f\"Random Tensor: \\n {x_rand} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"ybRocaNfGWkQ"},"source":["# Attributes of a Tensor\n","\n","\n","* Tensor attributes describe their __shape__, __datatype__, and the __device__ on which they are stored\n","* For __device__ specification see [CUDA](#CUDA) section\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yc0sUL95GWkQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648804232227,"user_tz":-120,"elapsed":395,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"bffd1952-be7a-442a-f71b-dd21ad10bc4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}],"source":["x = torch.rand(3,4)\n","\n","print(f\"Shape of tensor: {x.shape}\")\n","print(f\"Datatype of tensor: {x.dtype}\")\n","print(f\"Device tensor is stored on: {x.device}\")"]},{"cell_type":"markdown","metadata":{"id":"af5Bk9PNGWkR"},"source":["# Operations on Tensors\n","\n","\n","* Over __100 tensor operations__, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described [here](https://pytorch.org/docs/stable/torch.html)\n","* Each of these operations can be __run on the GPU__\n","* By default, tensors are created on the CPU and we need to __explicitly move tensors to the GPU__ \n","* Keep in mind that copying __large tensors__ across devices can be __expensive__ in terms of time and memory!"]},{"cell_type":"markdown","metadata":{"id":"qnko5NefVLkm"},"source":["  * **Elementwise operations**\n","\n","Some of the simplest and most useful operations\n","are the **elementwise** operations.\n","These apply a standard scalar operation\n","to each element of an array.\n","For functions that take two arrays as inputs,\n","elementwise operations apply some standard binary operator\n","on each pair of corresponding elements from the two arrays.\n","We can create an elementwise function from any function\n","that maps from a scalar to a scalar.\n","\n","In mathematical notation, we would denote such\n","a *unary* scalar operator (taking one input)\n","by the signature $f: \\mathbb{R} \\rightarrow \\mathbb{R}$.\n","This just means that the function is mapping\n","from any real number ($\\mathbb{R}$) onto another.\n","Likewise, we denote a *binary* scalar operator\n","(taking two real inputs, and yielding one output)\n","by the signature $f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$.\n","Given any two vectors $\\mathbf{u}$ and $\\mathbf{v}$ *of the same shape*,\n","and a binary operator $f$, we can produce a vector\n","$\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$\n","by setting $c_i \\gets f(u_i, v_i)$ for all $i$,\n","where $c_i, u_i$, and $v_i$ are the $i^\\mathrm{th}$ elements\n","of vectors $\\mathbf{c}, \\mathbf{u}$, and $\\mathbf{v}$.\n","Here, we produced the vector-valued\n","$F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\n","by **lifting** the scalar function to an elementwise vector operation.\n","\n","The common standard arithmetic operators\n","(`+`, `-`, `*`, `/`, and `**`)\n","have all been **lifted to elementwise operations\n","for any identically-shaped tensors of arbitrary shape**.\n","In the following example, we use commas to formulate a 5-element tuple,\n","where each element is the result of an elementwise operation.\n"]},{"cell_type":"code","source":["# Standard arithmetic operations (+, -, *, /, **) automatically *lifted* to elementwise opeations\n","x = torch.tensor([1,2,4,8]) #TRY setting dtype= torch.float32  OR dtype= torch.uint8\n","y = torch.tensor([0,3,5,7])\n","x+y, x-y, x*y, x/y, x**y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kb2My0j-dnko","executionInfo":{"status":"ok","timestamp":1648805757189,"user_tz":-120,"elapsed":691,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"decbefe8-8f91-4c91-c9b8-ce19d6afa0b5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 1.,  5.,  9., 15.]),\n"," tensor([ 1., -1., -1.,  1.]),\n"," tensor([ 0.,  6., 20., 56.]),\n"," tensor([   inf, 0.6667, 0.8000, 1.1429]),\n"," tensor([1.0000e+00, 8.0000e+00, 1.0240e+03, 2.0972e+06]))"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Other operation applied elementwise\n","\n","z = torch.exp(x)\n","z"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mbp76E2zfNT_","executionInfo":{"status":"ok","timestamp":1648805906085,"user_tz":-120,"elapsed":526,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"101337cd-b6df-4927-fd2e-11d734ad2fae"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"gWkaGav2GWkS"},"source":["* **Linear algebra operations**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dx_tb0YGWkT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648806699855,"user_tz":-120,"elapsed":512,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"8ea2c231-c901-424a-a9e0-6b9ffa156e17"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(82.)\n","tensor(82.)\n","tensor(82.)\n","tensor(82.)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n","  app.launch_new_instance()\n"]}],"source":["# These are 4 equivalent ways to computes the matrix multiplication between two tensors x and y. \n","\n","x = torch.tensor([1,2,4,8], dtype= torch.float32)\n","y = torch.tensor([0,3,5,7], dtype= torch.float32)\n","\n","z1 = x @ y\n","print(z1)\n","\n","z2 = x.matmul(y)\n","print(z2)\n","\n","z3 = torch.matmul(x, y)\n","print(z3)\n","\n","z4 = torch.rand_like(x)\n","torch.matmul(x, y, out=z4)\n","print(z4)"]},{"cell_type":"markdown","source":["* **Broadcasting mechanism**\n","\n","Allows to apply elementwise op. to matrices of different shapes:\n","replicate the rows and colums to make the 2 matrices of the same shape,\n","then apply the elementwise operation"],"metadata":{"id":"yKtMYBiPoVM4"}},{"cell_type":"code","source":["a = torch.arange(3).reshape((3, 1))\n","b = torch.arange(2).reshape((1, 2))\n","C = a + b\n","\n","print(f\" a: {a}, \\n b: {b}, \\n => a+b: {C} \")\n","\n","A = torch.arange(6).reshape((3, 2))\n","b = torch.arange(2).reshape((1, 2))\n","D = A + b\n","\n","print(f\" A: {A}, \\n b: {b}, \\n => A+b: {D}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dc0dyH2VoXlD","executionInfo":{"status":"ok","timestamp":1648809151824,"user_tz":-120,"elapsed":391,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"81f8d3cd-0f7e-4c93-cc8d-6d9cc9641f22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" a: tensor([[0],\n","        [1],\n","        [2]]), \n"," b: tensor([[0, 1]]), \n"," => a+b: tensor([[0, 1],\n","        [1, 2],\n","        [2, 3]]) \n"," A: tensor([[0, 1],\n","        [2, 3],\n","        [4, 5]]), \n"," b: tensor([[0, 1]]), \n"," => A+b: tensor([[0, 2],\n","        [2, 4],\n","        [4, 6]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"xoySjMg9GWkS"},"source":["# Joining tensors\n","\n","* You can use ``torch.cat`` to concatenate a sequence of tensors along a given dimension"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJNkEshaGWkS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648807249307,"user_tz":-120,"elapsed":556,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"73159964-6193-404b-cdc2-cdd8da08944e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 2., 4., 8., 0., 3., 5., 7.])\n","torch.Size([4]) torch.Size([4]) torch.Size([8])\n","tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [ 2.,  1.,  4.,  3.],\n","        [ 1.,  2.,  3.,  4.],\n","        [ 4.,  3.,  2.,  1.]])\n","tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n","        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n","        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n","torch.Size([6, 4]) torch.Size([3, 8])\n"]}],"source":["#concatenate 1D vectors:\n","t1 = torch.cat([x, y], dim=0)\n","print(t1)\n","print(x.shape, y.shape, t1.shape)\n","\n","#concatenate matrix:\n","X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n","Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n","T2= torch.cat((X, Y), dim=0)\n","T3 = torch.cat((X, Y), dim=1)\n","print(T2)\n","print(T3)\n","print(T2.shape, T3.shape)"]},{"cell_type":"markdown","metadata":{"id":"0Cs95BmOGWkS"},"source":["# Standard numpy-like indexing and slicing\n","\n","* Tensors use indexing notation, which also applies to standard Python lists\n","\n","<img src=\"https://raw.githubusercontent.com/giulianogrossi/imgs/main/pyTorch_tutorial_imgs/indexing.jpg\" alt=\"Drawing\" style=\"width: 70%\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G4j02aahVLkn","executionInfo":{"status":"ok","timestamp":1648807319047,"user_tz":-120,"elapsed":354,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"c749a3ba-6142-4c1c-bb31-4bf41ead6eb0"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 1, 2, 3, 4, 5]\n","[1, 2, 3]\n","[1, 2, 3, 4, 5]\n","[0, 1, 2, 3]\n","[0, 1, 2, 3, 4]\n","[1, 3]\n"]}],"source":["some_list = list(range(6)) \n","print(some_list[:]) \n","print(some_list[1:4])\n","print(some_list[1:])\n","print(some_list[:4]) \n","print(some_list[:-1])\n","print(some_list[1:4:2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcZ6kUZhGWkS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648809403681,"user_tz":-120,"elapsed":524,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"24875f1b-642f-4f22-d820-156366cf92bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["First row:  tensor([1., 1., 1., 1.])\n","First column:  tensor([1., 1., 1., 1.])\n","Last column: tensor([1., 1., 1., 1.])\n","tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"]}],"source":["x = torch.ones(4, 4)\n","print('First row: ',x[0])\n","print('First column: ', x[:, 0])\n","print('Last column:', x[..., -1])\n","x[:,1] = 0\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"9swHGLKwGWkT"},"source":["# Single-element tensors \n","\n","* If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using ``item()``\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_T05R0EGWkT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648807312223,"user_tz":-120,"elapsed":348,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"7727b298-8528-41bd-b6d6-92d201561f2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["15.0 <class 'float'>\n"]}],"source":["agg = x.sum()\n","agg_item = agg.item()  \n","print(agg_item, type(agg_item))"]},{"cell_type":"markdown","metadata":{"id":"IyweXVWHVLkp"},"source":["# Tensor element types\n","\n","* Numbers in Python are objects\n","* Lists in Python are meant for sequential collections of objects\n","* The Python interpreter is slow compared to optimized, compiled code\n","* Data science libraries rely on NumPy or introduce __dedicated data structures__ like PyTorch tensors, which provide efficient __low-level implementations__ of numerical data structures and __related operations__ on them, wrapped in a convenient high-level API"]},{"cell_type":"markdown","metadata":{"id":"t6jKdWOGVLkp"},"source":["## Data type\n","\n","* The `dtype` argument to tensor constructors specifies the __numerical data type__ that will be contained in the tensor\n","* Here’s a list of the possible values for the __dtype argument__: \n","  * `torch.float32` or `torch.float`: 32-bit floating-point\n","  * `torch.float64` or `torch.double`: 64-bit, double-precision floating-point \n","  * `torch.float16` or torch.half: 16-bit, half-precision floating-point \n","  * `torch.int8`: signed 8-bit integers \n","  * `torch.uint8`: unsigned 8-bit integers  \n","  * `torch.int16` or torch.short: signed 16-bit integers \n","  * `torch.int32` or torch.int: signed 32-bit integers  \n","  * `torch.int64` or torch.long: signed 64-bit integers \n","  * `torch.bool`: Boolean"]},{"cell_type":"markdown","metadata":{"id":"HuP-cPOVVLkp"},"source":["## Managing a tensor’s dtype attribute\n","\n","* In order to allocate a tensor of the right numeric type, we can __specify__ the __proper__ `dtype` as an argument to the constructor"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"X_PPONKpVLkq","executionInfo":{"status":"ok","timestamp":1648807343682,"user_tz":-120,"elapsed":497,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"b819a02a-0a96-4a78-8b01-15354bbbddff"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.int16\n","torch.float64\n"]}],"source":["double_points = torch.ones(10, 2, dtype=torch.double) \n","short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)\n","print(short_points.dtype)\n","print(double_points.dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OS0qLxWIVLkq","executionInfo":{"status":"ok","timestamp":1648807350765,"user_tz":-120,"elapsed":346,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"dd685f2b-30ce-41f8-fbe7-a5c6c83c06e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.int16\n","torch.float64\n"]}],"source":["# casting method, such as\n","double_points = torch.zeros(10, 2).double() \n","short_points = torch.ones(10, 2).short()\n","print(short_points.dtype)\n","print(double_points.dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCDOAV1FVLkq","executionInfo":{"status":"ok","timestamp":1648807382475,"user_tz":-120,"elapsed":540,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"84523180-b224-4b52-95e2-fa6fc8657802"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.int16\n","torch.float64\n"]}],"source":["# the more convenient \"to\" method:\n","double_points = torch.zeros(10, 2).to(torch.double) \n","short_points = torch.ones(10, 2).to(dtype=torch.short)\n","print(short_points.dtype)\n","print(double_points.dtype)"]},{"cell_type":"markdown","metadata":{"id":"RxegaK5IGWkT"},"source":["# In-place operations\n","\n","Observe:  x = x + 2 allocate new memory "]},{"cell_type":"code","source":["before = id(x) #x address before the operation\n","x = x+2 \n","after = id(x) #x address after the operation\n","print(before == after)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zKsYkeyjt-zd","executionInfo":{"status":"ok","timestamp":1648809898090,"user_tz":-120,"elapsed":528,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"7c0b1921-a672-4616-8884-05a082b3c4a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"markdown","source":["\n","* Operations that store the result into the operand are called in-place. They are denoted by a ``_`` suffix\n","* For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``\n"],"metadata":{"id":"c7BBlObouBn1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"euYY8cf1GWkU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648809938990,"user_tz":-120,"elapsed":565,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"d575b700-1eb1-4057-bfbc-fd864ddbbfb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[20., 19., 20., 20.],\n","        [20., 19., 20., 20.],\n","        [20., 19., 20., 20.],\n","        [20., 19., 20., 20.]]) \n","\n","tensor([[25., 24., 25., 25.],\n","        [25., 24., 25., 25.],\n","        [25., 24., 25., 25.],\n","        [25., 24., 25., 25.]])\n","True\n"]}],"source":["# In place using the suffix _before = id(x)\n","print(x, \"\\n\")\n","x.add_(5)\n","after = id(x)\n","print(x)\n","print(before == after) "]},{"cell_type":"code","source":["# In place using the assignement to preallocated x as x[:]\n","before = id(x)\n","x[:]  = x +  5\n","after = id(x)\n","print(before == after)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0bZmuk7Uuy4N","executionInfo":{"status":"ok","timestamp":1648810153576,"user_tz":-120,"elapsed":452,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"194a7faa-3990-4fbb-fe81-f1517b02c0c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["# In place using the operator +=\n","before = id(x)\n","x  +=  5\n","after = id(x)\n","print(before == after)"],"metadata":{"id":"glMDtwrXv58J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRyVQ9AXGWkU"},"source":["## Note\n","<div class=\"alert alert-info\">\n","<p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.</p>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"yF6vwaGCGWkU"},"source":["# Bridge with NumPy\n","\n","* Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change\tthe other\n","\n"]},{"cell_type":"markdown","metadata":{"id":"02H75eubGWkU"},"source":["Tensor to NumPy array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hD4a5BZGWkV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648810897753,"user_tz":-120,"elapsed":420,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"bf460546-313b-4dc0-9525-add304480f62"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([1., 1., 1., 1., 1.])\n","n: [1. 1. 1. 1. 1.]\n"]}],"source":["t = torch.ones(5)\n","print(f\"t: {t}\")\n","n = t.numpy()\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"U-XvzfMQGWkV"},"source":["NOTE: t and n **share the underlying memory**: \n","==> A change in the tensor reflects in the NumPy array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2kenRcf0GWkV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648810960005,"user_tz":-120,"elapsed":353,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"2ac81ef5-7977-4345-f712-ae59a8533fff"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([3., 3., 3., 3., 3.])\n","n: [3. 3. 3. 3. 3.]\n"]}],"source":["t.add_(1)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"l8AJLMrTGWkV"},"source":["## NumPy array to Tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMRe6ottGWkV"},"outputs":[],"source":["n = np.ones(5)\n","t = torch.from_numpy(n)"]},{"cell_type":"markdown","metadata":{"id":"HW0ti0FtGWkV"},"source":["Again, changes in the NumPy array reflects in the tensor.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GwqFOe-GGWkW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648810990664,"user_tz":-120,"elapsed":477,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"}},"outputId":"896959e1-4722-4578-b29a-507dafaf7b51"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n","n: [2. 2. 2. 2. 2.]\n"]}],"source":["np.add(n, 1, out=n)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"IK6_FLErVLkt"},"source":["# CUDA\n","\n","* PyTorch __tensors__ also can be __stored__ on a graphics processing unit (__GPU__) in order to perform massively parallel, fast computations\n","* All __operations__ that will be performed on the tensor will be carried out using __GPU-specific routines__ that come with PyTorch\n","* To call API CUDA use pycuda or numba"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfNSPcFiVLkt"},"outputs":[],"source":["# Check if GPU is available\n","print(torch.cuda.is_available())"]},{"cell_type":"markdown","metadata":{"id":"SgK9RfDHVLkt"},"source":["## Managing a tensor’s device attribute\n","\n","* PyTorch Tensor also has the notion of __device__, which is where on the computer the tensor data is placed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8x_924RpVLku"},"outputs":[],"source":["points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')\n","points_gpu"]},{"cell_type":"markdown","metadata":{"id":"yObCPolQVLku"},"source":["* We could instead __copy a tensor__ created on the CPU onto the GPU using the to method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3CU4ZcZVLku"},"outputs":[],"source":["points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n","points_gpu = points.to(device='cuda')\n","points_gpu"]},{"cell_type":"markdown","metadata":{"id":"zcW4c_a9VLku"},"source":["## Multiple GPUs\n","\n","* If our machine has __more__ than one __GPU__, we can also decide on which GPU we allocate the tensor by passing a __zero-based integer__ identifying the GPU on the machine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHLadhFnVLku"},"outputs":[],"source":["points_gpu = points.to(device='cuda:0')\n","points_gpu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcBRDdyEGWkR"},"outputs":[],"source":["# We move our tensor to the GPU if available\n","if torch.cuda.is_available():\n","  tensor = tensor.to('cuda')"]},{"cell_type":"markdown","metadata":{"id":"q9qDrso4VLkv"},"source":["* Any operation performed on the tensor, such as multiplying all elements by a constant, is carried out on the GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qho18VCeVLkv"},"outputs":[],"source":["# Multiplication performed on the CPU\n","points = 2 * points\n","\n","# Multiplication performed on the GPU\n","points_gpu = 2* points.to(device='cuda')\n","points_gpu"]},{"cell_type":"markdown","metadata":{"id":"YSY0HCz8VLkv"},"source":["* Note that the `points_gpu` tensor is not brought back to the CPU once the result has been computed\n","* In order to move the __tensor back__ to the CPU, we need to provide a __cpu argument__ to the `to` method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzHN9GhCVLkv"},"outputs":[],"source":["points_cpu = points_gpu.to(device='cpu')\n","points_cpu.device"]},{"cell_type":"markdown","metadata":{"id":"nGYfcN5AVLkv"},"source":["* We can also use the __shorthand methods__ cpu and cuda instead of the to method to achieve the same goal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qdsOWQzqVLkv"},"outputs":[],"source":["points_gpu = points.cuda() # default to GPU index 0\n","points_gpu = points.cuda(0) \n","points_cpu = points_gpu.cpu()\n"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"name":"pyTorch_tutorial.ipynb","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/b13fe25e4dd1e4b0fd5f5ce803bde74b/tensorqs_tutorial.ipynb","timestamp":1617879607584}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}