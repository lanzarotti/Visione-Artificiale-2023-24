{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab98LCJxGWkG",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Index\n",
    "\n",
    "* [PyTorch](#PyTorch)\n",
    "* [Tensors](#Tensors)\n",
    "* [Operations on Tensors](#Operations-on-Tensors)\n",
    "* [Tensors: Scenic views of storage*](#Tensors:-Scenic-views-of-storage-*)\n",
    "* [CUDA](#CUDA)\n",
    "\n",
    "\n",
    "\\* sections that can be skipped on first reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch\n",
    "\n",
    "* Integration with the rest of the scientific libraries in Python, such as [SciPy](https://www.scipy.org), [Scikit-learn](https://scikit-learn.org), and [Pandas](https://pandas.pydata.org)\n",
    "* Compared to NumPy arrays, PyTorch tensors perform very fast operations on graphical processing units (GPUs)\n",
    "  * distribute operations on multiple devices or machines\n",
    "  * keep track of the graph of computations that created them\n",
    "  * important features when implementing a modern deep learning library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRGUvEaaGWkM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensors \n",
    "\n",
    "* Tensors are a specialized __data structure__ that are very similar to __arrays__ and __matrices__\n",
    "* In __PyTorch__, we use tensors to encode the __inputs__ and __outputs__ of a model, as well as the modelâ€™s __parameters__\n",
    "* Tensors are similar to __NumPy__ ndarrays, except that tensors can run on __GPUs__\n",
    "* Tensors are also optimized for __automatic differentiation__\n",
    "* The __dimensionality__ of a tensor coincides with the __number of indexes__ used to refer to scalar values within the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![tensors](https://raw.githubusercontent.com/giulianogrossi/imgs/main/pyTorch_tutorial_imgs/tensors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The essence of tensors\n",
    "\n",
    "* Python __lists__ or __tuples__ of numbers are collections of Python objects that are __individually allocated__ in memory\n",
    "* PyTorch __tensors__ or __NumPy arrays__ are views over (typically) contiguous __memory blocks__ containing unboxed C numeric types rather than Python objects\n",
    "* Each element is (in general) a 32-bit (4-byte) __float__ or __int__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![memory](https://raw.githubusercontent.com/giulianogrossi/imgs/main/pyTorch_tutorial_imgs/memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5OOql-AGWkO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Initializing a Tensor\n",
    "\n",
    "\n",
    "* Tensors can be initialized in various ways, e.g. they can be created directly from data (the data type is automatically inferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibd4eOk-GWkN",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8Hkh3l1GWkO",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRH43kEKGWkO",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## From a NumPy array\n",
    "\n",
    "* Tensors can be created from NumPy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoC9y7rMGWkP",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9Y1kP_UGWkP",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## From another tensor\n",
    "\n",
    "* The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ty9JBAlBGWkP",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJfBJDrtGWkP",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## With random or constant values\n",
    "\n",
    "* ``shape`` is a tuple of tensor dimensions\n",
    "\n",
    "* Typically initialization are either with zeros, ones, some other constants, or numbers randomly sampled from a specific distribution. \n",
    "    * `zeros` allows to initialize the tensors explicitly to  0 \n",
    "    * `ones` allows to initialize the tensors explicitly to  1 \n",
    "    * `randn` initializes the matrix randomly sampling the elements values from a standard Gaussian (normal) distribution with a mean of 0 and a standard deviation of 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QezrxKPkGWkQ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "shape = (3,4)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "rand_tensor = torch.rand(shape)\n",
    "\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybRocaNfGWkQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attributes of a Tensor\n",
    "\n",
    "\n",
    "* Tensor attributes describe their __shape__, __datatype__, and the __device__ on which they are stored\n",
    "* For __device__ specification see [CUDA](#CUDA) section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yc0sUL95GWkQ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af5Bk9PNGWkR",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Operations on Tensors\n",
    "\n",
    "\n",
    "* Over __100 tensor operations__, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described [here](https://pytorch.org/docs/stable/torch.html)\n",
    "* Each of these operations can be __run on the GPU__\n",
    "* By default, tensors are created on the CPU and we need to __explicitly move tensors to the GPU__ \n",
    "* Keep in mind that copying __large tensors__ across devices can be __expensive__ in terms of time and memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Note\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<p>\n",
    "Some of the simplest and most useful operations\n",
    "are the *elementwise* operations.\n",
    "These apply a standard scalar operation\n",
    "to each element of an array.\n",
    "For functions that take two arrays as inputs,\n",
    "elementwise operations apply some standard binary operator\n",
    "on each pair of corresponding elements from the two arrays.\n",
    "We can create an elementwise function from any function\n",
    "that maps from a scalar to a scalar.\n",
    "\n",
    "In mathematical notation, we would denote such\n",
    "a *unary* scalar operator (taking one input)\n",
    "by the signature $f: \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "This just means that the function is mapping\n",
    "from any real number ($\\mathbb{R}$) onto another.\n",
    "Likewise, we denote a *binary* scalar operator\n",
    "(taking two real inputs, and yielding one output)\n",
    "by the signature $f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "Given any two vectors $\\mathbf{u}$ and $\\mathbf{v}$ *of the same shape*,\n",
    "and a binary operator $f$, we can produce a vector\n",
    "$\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$\n",
    "by setting $c_i \\gets f(u_i, v_i)$ for all $i$,\n",
    "where $c_i, u_i$, and $v_i$ are the $i^\\mathrm{th}$ elements\n",
    "of vectors $\\mathbf{c}, \\mathbf{u}$, and $\\mathbf{v}$.\n",
    "Here, we produced the vector-valued\n",
    "$F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\n",
    "by *lifting* the scalar function to an elementwise vector operation.\n",
    "\n",
    "The common standard arithmetic operators\n",
    "(`+`, `-`, `*`, `/`, and `**`)\n",
    "have all been *lifted* to elementwise operations\n",
    "for any identically-shaped tensors of arbitrary shape.\n",
    "We can call elementwise operations on any two tensors of the same shape.\n",
    "In the following example, we use commas to formulate a 5-element tuple,\n",
    "where each element is the result of an elementwise operation.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Cs95BmOGWkS",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Standard numpy-like indexing and slicing\n",
    "\n",
    "* Tensors use indexing notation, which also applies to standard Python lists\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/giulianogrossi/imgs/main/pyTorch_tutorial_imgs/indexing.jpg\" alt=\"Drawing\" style=\"width: 70%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "some_list = list(range(6)) \n",
    "print(some_list[:]) \n",
    "print(some_list[1:4])\n",
    "print(some_list[1:])\n",
    "print(some_list[:4]) \n",
    "print(some_list[:-1])\n",
    "print(some_list[1:4:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcZ6kUZhGWkS",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print('First row: ',tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoySjMg9GWkS",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joining tensors\n",
    "\n",
    "* You can use ``torch.cat`` to concatenate a sequence of tensors along a given dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJNkEshaGWkS",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWkaGav2GWkS",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Arithmetic operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dx_tb0YGWkT",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9swHGLKwGWkT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Single-element tensors \n",
    "\n",
    "* If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using ``item()``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_T05R0EGWkT",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()  \n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensor element types\n",
    "\n",
    "* Numbers in Python are objects\n",
    "* Lists in Python are meant for sequential collections of objects\n",
    "* The Python interpreter is slow compared to optimized, compiled code\n",
    "* Data science libraries rely on NumPy or introduce __dedicated data structures__ like PyTorch tensors, which provide efficient __low-level implementations__ of numerical data structures and __related operations__ on them, wrapped in a convenient high-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data type\n",
    "\n",
    "* The `dtype` argument to tensor constructors specifies the __numerical data type__ that will be contained in the tensor\n",
    "* Hereâ€™s a list of the possible values for the __dtype argument__: \n",
    "  * `torch.float32` or `torch.float`: 32-bit floating-point\n",
    "  * `torch.float64` or `torch.double`: 64-bit, double-precision floating-point \n",
    "  * `torch.float16` or torch.half: 16-bit, half-precision floating-point \n",
    "  * `torch.int8`: signed 8-bit integers \n",
    "  * `torch.uint8`: unsigned 8-bit integers  \n",
    "  * `torch.int16` or torch.short: signed 16-bit integers \n",
    "  * `torch.int32` or torch.int: signed 32-bit integers  \n",
    "  * `torch.int64` or torch.long: signed 64-bit integers \n",
    "  * `torch.bool`: Boolean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Managing a tensorâ€™s dtype attribute\n",
    "\n",
    "* In order to allocate a tensor of the right numeric type, we can __specify__ the __proper__ `dtype` as an argument to the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "double_points = torch.ones(10, 2, dtype=torch.double) \n",
    "short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)\n",
    "print(short_points.dtype)\n",
    "print(double_points.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# casting method, such as\n",
    "double_points = torch.zeros(10, 2).double() \n",
    "short_points = torch.ones(10, 2).short()\n",
    "print(short_points.dtype)\n",
    "print(double_points.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# the more convenient to method:\n",
    "double_points = torch.zeros(10, 2).to(torch.double) \n",
    "short_points = torch.ones(10, 2).to(dtype=torch.short)\n",
    "print(short_points.dtype)\n",
    "print(double_points.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensors: Scenic views of storage *\n",
    "\n",
    "* Values in tensors are allocated in __contiguous chunks__ of memory managed by `torch.Storage` instances\n",
    "* A storage is a __one-dimensional array__ of numerical data: a contiguous block of memory containing numbers of a given type, i.e.\n",
    "  * `float` (32 bits repre- senting a floating-point number)\n",
    "  * `int64` (64 bits representing an integer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![storage](https://raw.githubusercontent.com/giulianogrossi/imgs/main/pyTorch_tutorial_imgs/storage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tensor metadata\n",
    "* In order to index into a storage, tensors rely on a few pieces of information that, together with their storage, unequivocally define them:\n",
    " * __size__: is a tuple indicating how many elements across each dimension\n",
    " * __offset__: is the index in the storage corresponding to the first element in the tensor. \n",
    " * __stride__: is the number of elements in the storage that need to be skipped over to obtain the next element along each dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![memory](https://raw.githubusercontent.com/giulianogrossi/imgs/main/pyTorch_tutorial_imgs/stride.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxegaK5IGWkT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-place operations\n",
    "\n",
    "* Operations that store the result into the operand are called in-place. They are denoted by a ``_`` suffix\n",
    "* For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euYY8cf1GWkU",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRyVQ9AXGWkU",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Note\n",
    "<div class=\"alert alert-info\">\n",
    "<p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yF6vwaGCGWkU",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bridge with NumPy\n",
    "\n",
    "* Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change\tthe other\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02H75eubGWkU",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tensor to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hD4a5BZGWkV",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-XvzfMQGWkV",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A change in the tensor reflects in the NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kenRcf0GWkV",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8AJLMrTGWkV",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## NumPy array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMRe6ottGWkV",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HW0ti0FtGWkV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Changes in the NumPy array reflects in the tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwqFOe-GGWkW",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CUDA\n",
    "\n",
    "* PyTorch __tensors__ also can be __stored__ on a graphics processing unit (__GPU__) in order to perform massively parallel, fast computations\n",
    "* All __operations__ that will be performed on the tensor will be carried out using __GPU-specific routines__ that come with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Managing a tensorâ€™s device attribute\n",
    "\n",
    "* PyTorch Tensor also has the notion of __device__, which is where on the computer the tensor data is placed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')\n",
    "points_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We could instead __copy a tensor__ created on the CPU onto the GPU using the to method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points_gpu = points.to(device='cuda')\n",
    "points_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiple GPUs\n",
    "\n",
    "* If our machine has __more__ than one __GPU__, we can also decide on which GPU we allocate the tensor by passing a __zero-based integer__ identifying the GPU on the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "points_gpu = points.to(device='cuda:0')\n",
    "points_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcBRDdyEGWkR",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Any operation performed on the tensor, such as multiplying all elements by a constant, is carried out on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Multiplication performed on the CPU\n",
    "points = 2 * points\n",
    "\n",
    "# Multiplication performed on the GPU\n",
    "points_gpu = 2* points.to(device='cuda')\n",
    "points_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Note that the `points_gpu` tensor is not brought back to the CPU once the result has been computed\n",
    "* In order to move the __tensor back__ to the CPU, we need to provide a __cpu argument__ to the `to` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "points_cpu = points_gpu.to(device='cpu')\n",
    "points_cpu.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We can also use the __shorthand methods__ cpu and cuda instead of the to method to achieve the same goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "points_gpu = points.cuda() # default to GPU index 0\n",
    "points_gpu = points.cuda(0) \n",
    "points_cpu = points_gpu.cpu()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "Copia di tensorqs_tutorial.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/b13fe25e4dd1e4b0fd5f5ce803bde74b/tensorqs_tutorial.ipynb",
     "timestamp": 1617879607584
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
