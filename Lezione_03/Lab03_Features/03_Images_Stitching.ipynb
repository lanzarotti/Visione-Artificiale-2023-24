{"cells":[{"cell_type":"markdown","metadata":{"id":"XFhdUnqtoD0S"},"source":["# <font color='green'><b> Image Stitching </b></font>\n","\n","\n","https://towardsdatascience.com/image-panorama-stitching-with-opencv-2402bde6b46c"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#@title ▶️ Base dir setup\n","import os, sys\n","\n","# check if hosted (Google VM) or running on local server\n","if 'google.colab' in sys.modules:\n","  #@markdown Google Drive root folder - hosted by Google VM (adapt to your local paths)\n","  from google.colab import drive\n","  drive.mount('/content/drive', force_remount=False)\n","  base_dir = 'CV/' #@param {type: \"string\"}\n","  base_dir  = os.path.join('/content/drive/MyDrive/', base_dir)\n","  #!pip install pillow  --upgrade\n","  img_dir = 'data/img/'\n","  vid_dir = 'data/video/'\n","  out_dir = 'output/'\n","  \n","  # move to base_dir \n","  os.chdir(base_dir)\n","else:\n","  #@markdown Path to local folder on PC (adapt to your local paths)\n","  img_dir = '../data/img/'\n","  out_dir = '../data/output/'\n","\n"," \n","\n","print(\"Current dir:\", os.getcwd())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8054,"status":"ok","timestamp":1667561067202,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"},"user_tz":-60},"id":"EKNgfjdzE-r3"},"outputs":[],"source":["!pip3 install imutils "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":1716,"status":"ok","timestamp":1667561068912,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"},"user_tz":-60},"id":"lPg-3D3qFBXb","outputId":"f9e2d5ab-620d-4e13-c5bb-98367d86ae7c"},"outputs":[],"source":["import numpy as np\n","import cv2 \n","import os\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","from scipy import ndimage\n","from scipy.spatial import distance\n","from scipy.spatial import distance_matrix\n","import pickle\n","import math\n","import matplotlib.pyplot as plt\n","import imutils\n","from skimage.io import imread\n","from skimage.color import rgb2gray\n","from skimage import img_as_float, img_as_ubyte\n"," \n","\n","cv2.ocl.setUseOpenCL(False)\n","\n","cv2.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# read images and transform them to grayscale\n","# Make sure that the train image is the image that will be transformed\n"," \n","# load an example image \n","trainImg = plt.imread(img_dir + 'foto1A.jpg') #  yosemite2\n","trainImg_gray = img_as_ubyte(rgb2gray(trainImg))\n","\n","queryImg = plt.imread(img_dir +  'foto1B.jpg')#  yosemite1\n","queryImg_gray = img_as_ubyte(rgb2gray(queryImg))\n","\n","fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, constrained_layout=False, figsize=(16,9))\n","ax1.imshow(queryImg, cmap=\"gray\")\n","ax1.set_xlabel(\"Query image\", fontsize=14)\n","\n","ax2.imshow(trainImg, cmap=\"gray\")\n","ax2.set_xlabel(\"Train image (Image to be transformed)\", fontsize=14)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def detectAndDescribe(image):\n","    \"\"\"\n","    Compute key points and feature descriptors using the SIFT  method (alternatively: surf, brisk, orb)\n","    \"\"\"\n","    descriptor = cv2.SIFT_create()    \n","    # get keypoints and descriptors\n","    (kps, features) = descriptor.detectAndCompute(image, None)\n","    \n","    return (kps, features)\n","\n","kpsA, featuresA = detectAndDescribe(trainImg_gray)\n","kpsB, featuresB = detectAndDescribe(queryImg_gray)"]},{"cell_type":"markdown","metadata":{},"source":["### <font color='green'><b> EXERCISE: </b></font>\n"," \n","modify the function \"detectAndDescribe\" to allow the choice among SIFT, orb or brisk methods.\n","HINT: https://docs.opencv.org/4.x/"]},{"cell_type":"markdown","metadata":{},"source":["##### <font color='green'><b> - SOLUTION </b></font>\n","\n","[to be run instead of the previous cell]\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":268,"status":"ok","timestamp":1667561186981,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"},"user_tz":-60},"id":"ilgZQak8whTm"},"outputs":[],"source":["def detectAndDescribe(image, method=None):\n","    \"\"\"\n","    Compute key points and feature descriptors using an specific method\n","    \"\"\"\n","    \n","    assert method is not None, \"You need to define a feature detection method. Values are: 'sift', 'surf'\"\n","    \n","    # detect and extract features from the image\n","    if method == 'sift':\n","        descriptor = cv2.SIFT_create()\n","    elif method == 'brisk':\n","        descriptor = cv2.BRISK_create()\n","    elif method == 'orb':\n","        descriptor = cv2.ORB_create()\n","        \n","    # get keypoints and descriptors\n","    (kps, features) = descriptor.detectAndCompute(image, None)\n","    \n","    return (kps, features)\n","\n","# set the method you want to ues\n","feature_extractor = 'brisk' # one of 'sift', 'brisk', 'orb'\n","\n","kpsA, featuresA = detectAndDescribe(trainImg_gray, method=feature_extractor)\n","kpsB, featuresB = detectAndDescribe(queryImg_gray, method=feature_extractor)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"elapsed":1540,"status":"ok","timestamp":1667561351684,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"},"user_tz":-60},"id":"MPZrMglDwyUo","outputId":"a296d633-ee92-49f7-c6d2-37aa0d7991d1"},"outputs":[],"source":["# display the keypoints and features detected on both images\n","fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,8), constrained_layout=False)\n","ax1.imshow(cv2.drawKeypoints(trainImg_gray,kpsA,None,color=(0,255,0)))\n","ax1.set_xlabel(\"(a)\", fontsize=14)\n","ax2.imshow(cv2.drawKeypoints(queryImg_gray,kpsB,None,color=(0,255,0)))\n","ax2.set_xlabel(\"(b)\", fontsize=14)\n","print(len(kpsA))\n","print(len(kpsB))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":255,"status":"ok","timestamp":1667561456309,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"},"user_tz":-60},"id":"ROkj58ZxxM2o"},"outputs":[],"source":["def matchKeyPointsBF(featuresA, featuresB):\n","\n","    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True) \n","       \n","    # Match descriptors.\n","    best_matches = bf.match(featuresA,featuresB)\n","    \n","    # Sort the features in order of distance.\n","    # The points with small distance (more similarity) are ordered first in the vector\n","    rawMatches = sorted(best_matches, key = lambda x:x.distance)\n","    print(\"Raw matches (Brute force):\", len(rawMatches))\n","    return rawMatches"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":459},"executionInfo":{"elapsed":2802,"status":"ok","timestamp":1667561481339,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"},"user_tz":-60},"id":"TphF3dvlxbUy","outputId":"1de3d657-d5a2-4442-80a8-86b3aadcfdd9"},"outputs":[],"source":["fig = plt.figure(figsize=(20,8))\n"," \n","matches = matchKeyPointsBF(featuresA, featuresB)\n","img3 = cv2.drawMatches(trainImg,kpsA,queryImg,kpsB,matches[:50], None)\n"," \n","\n","plt.imshow(img3)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":238,"status":"ok","timestamp":1667561546234,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"},"user_tz":-60},"id":"4rUWeT5xx8vI"},"outputs":[],"source":["def getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh):\n","    # convert the keypoints to numpy arrays\n","    kpsA = np.float32([kp.pt for kp in kpsA])\n","    kpsB = np.float32([kp.pt for kp in kpsB])\n","    \n","    if len(matches) > 4:\n","\n","        # construct the two sets of points\n","        ptsA = np.float32([kpsA[m.queryIdx] for m in matches])\n","        ptsB = np.float32([kpsB[m.trainIdx] for m in matches])\n","        \n","        # estimate the homography between the sets of points\n","        (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,\n","            reprojThresh)\n","\n","        return (matches, H, status)\n","    else:\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1667561629321,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"},"user_tz":-60},"id":"EoBJ5lL3yBUe","outputId":"2470eef8-b1dc-42ce-ab86-245f7fef26cd"},"outputs":[],"source":["M = getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh=4)\n","if M is None:\n","    print(\"Error!\")\n","(matches, H, status) = M\n","print(H)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":575},"executionInfo":{"elapsed":1572,"status":"ok","timestamp":1667561649040,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"},"user_tz":-60},"id":"pKEyMwugyGjn","outputId":"b40eee01-01fa-4189-f370-8343a2a61576"},"outputs":[],"source":["# Apply panorama correction\n","width = trainImg.shape[1] + queryImg.shape[1]\n","height = trainImg.shape[0] + queryImg.shape[0]\n","\n","\n","result = cv2.warpPerspective(trainImg, H, (width, height))\n","result[0:queryImg.shape[0], 0:queryImg.shape[1]] = queryImg\n","\n","plt.figure(figsize=(20,10))\n","plt.imshow(result)\n","\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":567},"executionInfo":{"elapsed":2189,"status":"ok","timestamp":1667561725186,"user":{"displayName":"Raffaella Lanzarotti","userId":"13623836324684543005"},"user_tz":-60},"id":"GkM8w6riyasi","outputId":"1efd6209-93fc-4ffc-9a82-6864bf2af4e6"},"outputs":[],"source":["# transform the panorama image to grayscale and threshold it \n","gray = img_as_ubyte(rgb2gray(result))\n","thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1]\n","\n","# Finds contours from the binary image\n","cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","cnts = imutils.grab_contours(cnts)\n","\n","# get the maximum contour area\n","c = max(cnts, key=cv2.contourArea)\n","\n","# get a bbox from the contour area\n","(x, y, w, h) = cv2.boundingRect(c)\n","\n","# crop the image to the bbox coordinates\n","result = result[y:y + h, x:x + w]\n","\n","# show the cropped image\n","plt.figure(figsize=(20,10))\n","plt.imshow(result)\n"]},{"cell_type":"markdown","metadata":{},"source":["### <font color='blue'><b> EXTENSION: </b></font>\n","  \n","Would you be able to generalize and stitch more than two images? HINT: choose an image ar target and tranform the other to that reference system"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMX/NCVYojimpSZKscofJsq","collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
